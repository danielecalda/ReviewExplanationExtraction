{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from csv and splitting\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "import progressbar\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "DATA_FILE1 = '../data/nouns_list.pkl'\n",
    "DATA_FILE2 = '../data/pos_word_list.pkl'\n",
    "DATA_FILE3 = '../data/neu_word_list.pkl'\n",
    "DATA_FILE4 = '../data/neg_word_list.pkl'\n",
    "DATA_FILE5 = '../data/others_list.pkl'\n",
    "DATA_FILE6 = '../data/verbs_list.pkl'\n",
    "\n",
    "train_list = []\n",
    "dev_list = []\n",
    "test_list = []\n",
    "\n",
    "print(\"Reading from csv and splitting\")\n",
    "for i, line in enumerate(open('../data/reviews200k.json', 'r')):\n",
    "    if i < 100000 and len(line) > 300:\n",
    "        train_list.append(json.loads(line))\n",
    "    if 99999 < i < 100500 and len(line) > 300:\n",
    "        dev_list.append(json.loads(line))\n",
    "    if 149999 < i < 200000 and len(line) > 300:\n",
    "        test_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = []\n",
    "for i in range(0, 6):\n",
    "    j = 0\n",
    "    for line in train_list:\n",
    "        if i == int(line['stars']):\n",
    "            train_reviews.append(line)\n",
    "            j = j + 1\n",
    "        if j > 10:\n",
    "            break\n",
    "\n",
    "train_examples = [review['text'][0:100].lower() for review in train_reviews]\n",
    "train_labels = [int(review['stars']) for review in train_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total bill for this horrible service? over $8gs. these crooks actually had the nerve to charge us $6\n",
      "today was my second out of three sessions i had paid for. although my first session went well, i cou\n"
     ]
    }
   ],
   "source": [
    "print(train_examples[0])\n",
    "print(train_examples[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_polarity(word):\n",
    "    if (sid.polarity_scores(word)['compound']) >= 0.1:\n",
    "        return 'POS_JJ'\n",
    "    elif (sid.polarity_scores(word)['compound']) <= -0.1:\n",
    "        return 'NEG_JJ'\n",
    "    else:\n",
    "        return 'NEU_JJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (55 of 55) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_examples = []\n",
    "train_examples_pos = []\n",
    "\n",
    "for example in progressbar.progressbar(train_examples):\n",
    "    doc = spacy_nlp(example)\n",
    "    \n",
    "    pos_example = []\n",
    "    tokenized_example = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.pos_ not in 'PUNCT':\n",
    "            if token.pos_ == 'ADJ':\n",
    "                pos_example.append(set_polarity(token.text))\n",
    "                tokenized_example.append(token.text)\n",
    "            else:\n",
    "                pos_example.append(token.tag_)\n",
    "                tokenized_example.append(token.text)\n",
    "                \n",
    "    train_examples_pos.append(pos_example)\n",
    "    tokenized_train_examples.append(tokenized_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total', 'bill', 'for', 'this', 'horrible', 'service', 'over', '$', '8gs', 'these', 'crooks', 'actually', 'had', 'the', 'nerve', 'to', 'charge', 'us', '$', '6']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NEU_JJ', 'NN', 'IN', 'DT', 'NEG_JJ', 'NN', 'IN', '$', 'NN', 'DT', 'NNS', 'RB', 'VBD', 'DT', 'NN', 'TO', 'VB', 'PRP', '$', 'CD']\n"
     ]
    }
   ],
   "source": [
    "print(train_examples_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 4 rules\n"
     ]
    }
   ],
   "source": [
    "from parsing.text_base import rules_review\n",
    "from parsing.text_base import rules_review\n",
    "from parsing.grammar import Grammar\n",
    "\n",
    "grammar = Grammar(rules_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |#                                                 | 54 Elapsed Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "from parsing.grammar import add_rule\n",
    "from parsing.rule import Rule, rule_not_in_list\n",
    "\n",
    "rules = []\n",
    "rules_tag = []\n",
    "\n",
    "for train_example_pos, tokenized_example in progressbar.progressbar(zip(train_examples_pos, tokenized_train_examples)):\n",
    "    for pos, text in zip(train_example_pos, tokenized_example):\n",
    "        rule = Rule('$' + pos, text)\n",
    "        rule_tag = Rule('$Optional', '$' + pos)\n",
    "        if rule_not_in_list(rule, rules):\n",
    "            rules.append(rule)\n",
    "        if rule_not_in_list(rule_tag, rules_tag):\n",
    "            rules_tag.append(rule_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(random_number, lenght):\n",
    "    if length == 2:\n",
    "        return [random_number, random_number + 1, random_number + 2]\n",
    "    elif length == 3:\n",
    "        return [random_number, random_number + 1, random_number + 2]\n",
    "    else:\n",
    "        return [random_number, random_number + 1, random_number + 2, random_number + 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_sequences(sentence_length, random_length):\n",
    "    sequences = []\n",
    "    for i in range(2, 4):\n",
    "        random_number = random.choice(range(0, sentence_length - 2))\n",
    "        sequences.append(generate_sequence(random_number, i))\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |#                                                 | 54 Elapsed Time: 0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16, 17, 18], [3, 4, 5]]\n",
      "[[16, 17, 18], [4, 5, 6]]\n",
      "[[3, 4, 5], [5, 6, 7]]\n",
      "[[10, 11, 12], [9, 10, 11]]\n",
      "[[1, 2, 3], [10, 11, 12]]\n",
      "[[12, 13, 14], [7, 8, 9]]\n",
      "[[15, 16, 17], [14, 15, 16]]\n",
      "[[15, 16, 17], [8, 9, 10]]\n",
      "[[11, 12, 13], [12, 13, 14]]\n",
      "[[7, 8, 9], [10, 11, 12]]\n",
      "[[14, 15, 16], [9, 10, 11]]\n",
      "[[2, 3, 4], [9, 10, 11]]\n",
      "[[13, 14, 15], [13, 14, 15]]\n",
      "[[4, 5, 6], [5, 6, 7]]\n",
      "[[8, 9, 10], [6, 7, 8]]\n",
      "[[13, 14, 15], [13, 14, 15]]\n",
      "[[3, 4, 5], [5, 6, 7]]\n",
      "[[17, 18, 19], [15, 16, 17]]\n",
      "[[11, 12, 13], [13, 14, 15]]\n",
      "[[21, 22, 23], [11, 12, 13]]\n",
      "[[0, 1, 2], [3, 4, 5]]\n",
      "[[13, 14, 15], [9, 10, 11]]\n",
      "[[5, 6, 7], [8, 9, 10]]\n",
      "[[11, 12, 13], [8, 9, 10]]\n",
      "[[6, 7, 8], [0, 1, 2]]\n",
      "[[16, 17, 18], [5, 6, 7]]\n",
      "[[6, 7, 8], [7, 8, 9]]\n",
      "[[7, 8, 9], [9, 10, 11]]\n",
      "[[1, 2, 3], [8, 9, 10]]\n",
      "[[18, 19, 20], [5, 6, 7]]\n",
      "[[4, 5, 6], [10, 11, 12]]\n",
      "[[10, 11, 12], [3, 4, 5]]\n",
      "[[14, 15, 16], [6, 7, 8]]\n",
      "[[13, 14, 15], [19, 20, 21]]\n",
      "[[13, 14, 15], [7, 8, 9]]\n",
      "[[13, 14, 15], [12, 13, 14]]\n",
      "[[4, 5, 6], [9, 10, 11]]\n",
      "[[12, 13, 14], [6, 7, 8]]\n",
      "[[9, 10, 11], [11, 12, 13]]\n",
      "[[13, 14, 15], [8, 9, 10]]\n",
      "[[12, 13, 14], [0, 1, 2]]\n",
      "[[11, 12, 13], [0, 1, 2]]\n",
      "[[4, 5, 6], [13, 14, 15]]\n",
      "[[11, 12, 13], [13, 14, 15]]\n",
      "[[5, 6, 7], [9, 10, 11]]\n",
      "[[9, 10, 11], [2, 3, 4]]\n",
      "[[5, 6, 7], [12, 13, 14]]\n",
      "[[5, 6, 7], [16, 17, 18]]\n",
      "[[14, 15, 16], [4, 5, 6]]\n",
      "[[14, 15, 16], [16, 17, 18]]\n",
      "[[16, 17, 18], [4, 5, 6]]\n",
      "[[13, 14, 15], [10, 11, 12]]\n",
      "[[13, 14, 15], [10, 11, 12]]\n",
      "[[2, 3, 4], [10, 11, 12]]\n",
      "[[1, 2, 3], [11, 12, 13]]\n"
     ]
    }
   ],
   "source": [
    "for train_example_pos, tokenized_example in progressbar.progressbar(zip(train_examples_pos, tokenized_train_examples)):\n",
    "    sequences = generate_random_sequences(len(train_example_pos), 3, 2)\n",
    "    print(sequences)\n",
    "    for sequence in sequences:\n",
    "        rhs = '$' + train_example_pos[sequence[0]]\n",
    "        sem = '' + train_example_pos[sequence[0]]\n",
    "        for i in range(1,len(sequence)):\n",
    "            rhs = rhs + ' $' + train_example_pos[sequence[i]]\n",
    "            sem = sem + ' ' + train_example_pos[sequence[i]]\n",
    "        rule = Rule('$ReviewPattern', rhs, sem)\n",
    "        rules.append(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rule in rules:\n",
    "    add_rule(grammar, rule)\n",
    "for rule in rules_tag:\n",
    "    add_rule(grammar, rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom parsing.grammar import print_grammar\\n\\nprint_grammar(grammar)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from parsing.grammar import print_grammar\n",
    "\n",
    "print_grammar(grammar)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| |        #                                          | 3 Elapsed Time: 0:00:50"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "semantics_tuples = []\n",
    "\n",
    "for example, label in progressbar.progressbar(zip(train_examples, train_labels)):\n",
    "    s = re.sub(r'[^\\w\\s]','',example)\n",
    "\n",
    "    parses = grammar.parse_input(s)\n",
    "    \n",
    "    semantics = []\n",
    "    for parse in parses:\n",
    "        if parse.semantics not in semantics:\n",
    "            semantics.append(parse.semantics)\n",
    "            \n",
    "    for semantic in semantics:\n",
    "        semantics_tuples.append((semantic, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(semantics_tuples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:babble] *",
   "language": "python",
   "name": "conda-env-babble-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
